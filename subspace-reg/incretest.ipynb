{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_option\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "print(\"parse_option\")\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models import model_pool\n",
    "from models.util import create_model\n",
    "from models.resnet_language import LangPuller\n",
    "\n",
    "\n",
    "from dataset.mini_imagenet import ImageNet, MetaImageNet\n",
    "from dataset.tiered_imagenet import TieredImageNet, MetaTieredImageNet\n",
    "from dataset.transform_cfg import transforms_options, transforms_list\n",
    "\n",
    "from util import adjust_learning_rate, create_and_save_embeds, create_and_save_descriptions\n",
    "from eval.util import accuracy, AverageMeter, validate\n",
    "from configs import parse_option_supervised\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cub200(Dataset):\n",
    "    #index_path -> txt_path, \n",
    "    #index -> base size\n",
    "    def __init__(self, args,root='./cub', train=True,\n",
    "                 index_path=None, index=None, base_sess=None,transform=None,):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.base_sess = base_sess\n",
    "        self.transform = transform\n",
    "        self.index_path = index_path\n",
    "        self.index = index\n",
    "\n",
    "        self.train = train  # training set or test set\n",
    "        self._pre_operate()\n",
    "        self.mean = [120.39586422 / 255.0, 115.59361427 / 255.0, 104.54012653 / 255.0]\n",
    "        self.std = [70.68188272 / 255.0, 68.27635443 / 255.0, 72.54505529 / 255.0]\n",
    "        self.normalize = transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        self.unnormalize = transforms.Normalize(mean=-np.array(self.mean)/self.std, std=1/np.array(self.std))\n",
    "\n",
    "        if transform is None:\n",
    "            if self.base_sess == True:\n",
    "                self.transform = transforms.Compose([\n",
    "                    lambda x: Image.fromarray(x),\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    lambda x: np.asarray(x),\n",
    "                    transforms.ToTensor(),\n",
    "                    self.normalize\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                lambda x: Image.fromarray(x),\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                lambda x: np.asarray(x),\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        if self.train:\n",
    "            # self.data, self.targets = self.SelectfromTxt(self.data2label, index_path)\n",
    "            #base는 100까지 따라서 index = 100\n",
    "            if base_sess:\n",
    "                self.data, self.targets = self.SelectfromClasses(self.data, self.targets, index)\n",
    "            #novel session에 대한 세션 정보 줘야함\n",
    "            else:\n",
    "                self.data, self.targets = self.SelectfromTxt(self.data2label, index_path)\n",
    "        else:\n",
    "            if base_sess:\n",
    "                print(index)\n",
    "                self.data, self.targets = self.SelectfromClasses(self.data, self.targets, index)\n",
    "            else:\n",
    "                #modifying\n",
    "                #novel, test\n",
    "                self.data, self.targets = self.SelectfromNovelClasses(self.data, self.targets, index_path)\n",
    "                       \n",
    "            #HSJ self.labels\n",
    "        self.labels = self.targets\n",
    "        self.imgs = self._getImg(self.data)\n",
    "            #HSJ self.imgs\n",
    "        #HSJ LABELTOHUMAN\n",
    "\n",
    "        # Labels are available by codes by default. Converting them into human readable labels.\n",
    "        self.label2human =[\"\"] *200\n",
    "        with open('./cub/CUB_200_2011/' +'classes.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                catname, humanname = line.strip().lower().split(' ')\n",
    "                num,humanname = humanname.strip().lower().split('.')\n",
    "                humanname = \" \".join(humanname.split('_'))\n",
    "                if int(catname) in range(1,201):\n",
    "                    self.label2human[int(catname)-1]= humanname\n",
    "        #HSJ LABELTOHUMAN\n",
    "\n",
    "        #HSJ basec_map\n",
    "        basec = np.sort(np.arange(100))\n",
    "                \n",
    "        # Create mapping for base classes as they are not consecutive anymore.\n",
    "        self.basec_map = dict(zip(basec, np.arange(len(basec))))\n",
    "        #HSJ basec_map\n",
    "\n",
    "    def _getImg(self,d_list):\n",
    "        img_list = []\n",
    "        for d_path in d_list:\n",
    "            c_img = Image.open(d_path).convert('RGB')\n",
    "            c_img = np.array(c_img)\n",
    "            c_img_transformed = self.transform(c_img)\n",
    "            img_list.append(c_img_transformed.numpy())\n",
    "        img_list_np = np.array(img_list)\n",
    "        return img_list_np\n",
    "\n",
    "        \n",
    "    def SelectfromTxt(self, data2label, index_path):\n",
    "        index = open('./cub/CUB_200_2011/index_list/session_'+ str(index_path) + '.txt').read().splitlines()\n",
    "        data_tmp = []\n",
    "        targets_tmp = []\n",
    "        for i in index:\n",
    "            img_path = self.root + '/'+ i\n",
    "            data_tmp.append(img_path)\n",
    "            targets_tmp.append(data2label[img_path])\n",
    "\n",
    "        return data_tmp, targets_tmp\n",
    "\n",
    "    def SelectfromClasses(self, data, targets, index):\n",
    "        data_tmp = []\n",
    "        targets_tmp = []\n",
    "        for i in range(index):\n",
    "            ind_cl = np.where(i == targets)[0]\n",
    "            for j in ind_cl:\n",
    "                data_tmp.append(data[j])\n",
    "                targets_tmp.append(targets[j])\n",
    "        return data_tmp, targets_tmp\n",
    "    \n",
    "    def SelectfromNovelClasses(self, data, targets,index_path):\n",
    "        data_tmp = []\n",
    "        targets_tmp = []\n",
    "        for i in range(100+((index_path-2)*10),100+((index_path-1)*10)):\n",
    "            ind_cl = np.where(i == targets)[0]\n",
    "            for j in ind_cl:\n",
    "                data_tmp.append(data[j])\n",
    "                targets_tmp.append(targets[j])\n",
    "\n",
    "        return data_tmp, targets_tmp\n",
    "    def list2dict(self, list):\n",
    "        dict = {}\n",
    "        for l in list:\n",
    "            s = l.split(' ')\n",
    "            id = int(s[0])\n",
    "            cls = s[1]\n",
    "            if id not in dict.keys():\n",
    "                dict[id] = cls\n",
    "            else:\n",
    "                raise EOFError('The same ID can only appear once')\n",
    "        return dict\n",
    "    \n",
    "    def text_read(self, file):\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                lines[i] = line.strip('\\n')\n",
    "        return lines\n",
    "        \n",
    "    def _pre_operate(self):\n",
    "            image_file = './cub/'+ 'CUB_200_2011/images.txt'\n",
    "            split_file = './cub/'+ 'CUB_200_2011/train_test_split.txt'\n",
    "            class_file = './cub/'+ 'CUB_200_2011/image_class_labels.txt'\n",
    "            id2image = self.list2dict(self.text_read(image_file))\n",
    "            id2train = self.list2dict(self.text_read(split_file))  # 1: train images; 0: test iamges\n",
    "            id2class = self.list2dict(self.text_read(class_file))\n",
    "            train_idx = []\n",
    "            test_idx = []\n",
    "            for k in sorted(id2train.keys()):\n",
    "                if id2train[k] == '1':\n",
    "                    train_idx.append(k)\n",
    "                else:\n",
    "                    test_idx.append(k)\n",
    "\n",
    "            self.data = []\n",
    "            self.targets = []\n",
    "            self.data2label = {}\n",
    "            if self.train:\n",
    "                for k in train_idx:\n",
    "                    image_path = './cub/'+ 'CUB_200_2011/images/'+ str(id2image[k])\n",
    "                    self.data.append(image_path)\n",
    "                    self.targets.append(int(id2class[k]) - 1)\n",
    "                    self.data2label[image_path] = (int(id2class[k]) - 1)\n",
    "\n",
    "            else:\n",
    "                for k in test_idx:\n",
    "                    image_path = './cub/'+ 'CUB_200_2011/images/'+ str(id2image[k])\n",
    "                    self.data.append(image_path)\n",
    "                    self.targets.append(int(id2class[k]) - 1)\n",
    "                    self.data2label[image_path] = (int(id2class[k]) - 1)\n",
    "            self.targets = np.array(self.targets)\n",
    "                    \n",
    "    def __getitem__(self, item):\n",
    "        if self.base_sess:\n",
    "            img = self.imgs[item]\n",
    "            target = self.targets[item] - min(self.labels)\n",
    "            \n",
    "            return img, target,item\n",
    "        else:\n",
    "            if self.train == True and self.base_sess and self.n_base_support_samples > 0:\n",
    "                    assert self.n_base_support_samples > 0\n",
    "                    # These samples will be stored in memory for every episode.\n",
    "                    support_xs = []\n",
    "                    support_ys = []\n",
    "                    if self.fix_seed:\n",
    "                        np.random.seed(item)\n",
    "                    cls_sampled = np.random.choice(self.classes, len(self.classes), False)\n",
    "                    \n",
    "                    for idx, cls in enumerate(np.sort(cls_sampled)):\n",
    "                        imgs = np.asarray(self.data[cls]).astype('uint8')\n",
    "                        support_xs_ids_sampled = np.random.choice(range(imgs.shape[0]),\n",
    "                                                                  self.n_base_support_samples,\n",
    "                                                                  False)\n",
    "                        support_xs.append(imgs[support_xs_ids_sampled])\n",
    "                        support_ys.append([cls] * self.n_base_support_samples)    \n",
    "                    support_xs, support_ys = np.array(support_xs), np.array(support_ys)\n",
    "                    num_ways, n_queries_per_way, height, width, channel = support_xs.shape\n",
    "                    support_xs = support_xs.reshape((-1, height, width, channel))\n",
    "                    if self.n_base_aug_support_samples > 1:\n",
    "                        support_xs = np.tile(support_xs, (self.n_base_aug_support_samples, 1, 1, 1))\n",
    "                        support_ys = np.tile(support_ys.reshape((-1, )), (self.n_base_aug_support_samples))\n",
    "                    support_xs = np.split(support_xs, support_xs.shape[0], axis=0)\n",
    "                    support_xs = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs)))\n",
    "\n",
    "                    # Dummy query.\n",
    "                    query_xs = support_xs\n",
    "                    query_ys = support_ys\n",
    "            else:\n",
    "            \n",
    "                if self.fix_seed:\n",
    "                    np.random.seed(item)\n",
    "\n",
    "                #몇개로 나눌지(cub는 의미 없음)\n",
    "                \"\"\"BytesWarning\n",
    "                if self.disjoint_classes:\n",
    "                    cls_sampled = self.classes[:self.n_ways] # \n",
    "                    self.classes = self.classes[self.n_ways:]\n",
    "                else:\n",
    "                    cls_sampled = np.random.choice(self.classes, self.n_ways, False)\n",
    "                \"\"\"\n",
    "                cls_sampled = self.targets\n",
    "\n",
    "                support_xs = []\n",
    "                support_ys = []\n",
    "                query_xs = []\n",
    "                query_ys = []\n",
    "                for idx, cls in enumerate(np.sort(cls_sampled)):\n",
    "                    #support_xs_ids_sampled = np.random.choice(range(imgs.shape[0]), self.n_shots, False)\n",
    "                    support_xs.append(self.imgs)\n",
    "                    #support_xs.append(imgs[support_xs_ids_sampled])\n",
    "                    lbl = idx\n",
    "                    if self.eval_mode in [\"few-shot-incremental-fine-tune\"]:\n",
    "                        lbl = cls\n",
    "                    support_ys.append([lbl] * self.n_shots) #\n",
    "\n",
    "                    #query_xs_ids = np.setxor1d(np.arange(imgs.shape[0]), support_xs_ids_sampled)\n",
    "                    #query_xs_ids = np.random.choice(query_xs_ids, self.n_queries, False)\n",
    "                    query_xs.append(self.imgs)\n",
    "                    #query_xs.append(imgs[query_xs_ids])\n",
    "                    query_ys.append([lbl] * 30) #\n",
    "\n",
    "                support_xs, support_ys, query_xs, query_ys = np.array(support_xs), np.array(support_ys), np.array(query_xs), np.array(query_ys)\n",
    "                num_ways, n_queries_per_way, height, width, channel = query_xs.shape\n",
    "\n",
    "                query_xs = query_xs.reshape((num_ways * n_queries_per_way, height, width, channel))\n",
    "                query_ys = query_ys.reshape((num_ways * n_queries_per_way, ))\n",
    "\n",
    "                support_xs = support_xs.reshape((-1, height, width, channel))\n",
    "                \"\"\"\n",
    "                if self.n_aug_support_samples > 1:\n",
    "                    support_xs = np.tile(support_xs, (self.n_aug_support_samples, 1, 1, 1))\n",
    "                    support_ys = np.tile(support_ys.reshape((-1, )), (self.n_aug_support_samples))\n",
    "                \"\"\"\n",
    "                support_xs = np.split(support_xs, support_xs.shape[0], axis=0)\n",
    "                query_xs = query_xs.reshape((-1, height, width, channel))\n",
    "                query_xs = np.split(query_xs, query_xs.shape[0], axis=0)\n",
    "\n",
    "                support_xs = torch.stack(list(map(lambda x: self.train_transform(x.squeeze()), support_xs)))\n",
    "                query_xs = torch.stack(list(map(lambda x: self.test_transform(x.squeeze()), query_xs)))\n",
    "\n",
    "        return support_xs.float(), support_ys, query_xs.float(), query_ys\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = lambda x: None\n",
    "args.n_ways = 5\n",
    "args.n_shots = 5\n",
    "args.n_queries = 30\n",
    "args.data_root = 'data'\n",
    "args.data_aug = True\n",
    "args.n_test_runs = 5\n",
    "args.n_aug_support_samples = 1\n",
    "args.set_seed = 20\n",
    "args.continual = True\n",
    "args.eval_mode = \"few-shot-incremental-fine-tune\"\n",
    "args.n_base_support_samples = 1\n",
    "args.n_base_aug_support_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_loader = DataLoader(ImageNet(args=args, split='train', phase='test'),\n",
    "                                      batch_size=64 // 2,\n",
    "                                      shuffle=False,\n",
    "                                      drop_last=False,\n",
    "                                      num_workers=5 // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_valloader = DataLoader(MetaImageNet(args=args, split='val',\n",
    "                                                 \n",
    "                                                 fix_seed=True, use_episodes=False, disjoint_classes=True),\n",
    "                                    batch_size=64, shuffle=False, drop_last=False,\n",
    "                                    num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_valloader_it = itertools.cycle(iter(meta_valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_valloader_it = itertools.cycle(iter(base_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_batch = next(base_valloader_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([57, 18, 48, 38, 27, 18, 15, 58, 14, 24, 36, 28, 13, 26, 18, 28, 43, 30,\n",
       "        52,  9, 26,  4, 10, 30, 40, 53, 45, 33, 31, 12,  3, 23])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_a_dim(data): #TODO why do we need this in the first place?\n",
    "    support_xs, support_ys, query_xs, query_ys = data\n",
    "    batch_size, _, height, width, channel = support_xs.size()\n",
    "    support_xs = support_xs.view(-1, height, width, channel)\n",
    "    query_xs = query_xs.view(-1, height, width, channel)\n",
    "    support_ys = support_ys.view(-1).detach().numpy() # TODO\n",
    "    query_ys = query_ys.view(-1).detach().numpy()\n",
    "    return (support_xs, support_ys, query_xs, query_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_idx = drop_a_dim(next(meta_valloader_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 13, 17, 18, 20, 22, 25, 26, 28, 34, 40, 43, 58, 61, 67, 71, 73,\n",
       "       78, 81, 85, 86, 90, 95, 97, 99])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.unique(d_idx[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([name for name in meta_valloader.dataset.label2human if name != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabs(base_loader=None, novel_loader=None, query_ys=None):\n",
    "    vocab_all = []\n",
    "    vocab_base = None\n",
    "    if base_loader is not None:\n",
    "        label2human_base = base_loader.dataset.label2human\n",
    "        vocab_base  = [name for name in label2human_base if name != '']\n",
    "        vocab_all  += vocab_base\n",
    "\n",
    "    vocab_novel, orig2id = None, None\n",
    "\n",
    "    if novel_loader is not None:\n",
    "        novel_ids = np.sort(np.unique(query_ys))\n",
    "        label2human_novel = novel_loader.dataset.label2human\n",
    "        vocab_novel = [label2human_novel[i] for i in novel_ids]\n",
    "        orig2id = dict(zip(novel_ids, len(vocab_base) + np.arange(len(novel_ids))))\n",
    "        vocab_all += vocab_novel\n",
    "\n",
    "    return vocab_base, vocab_all, vocab_novel, orig2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_vocabs = get_vocabs(train_test,val_test,d_idx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parakeet auklet',\n",
       " 'indigo bunting',\n",
       " 'spotted catbird',\n",
       " 'gray catbird',\n",
       " 'eastern towhee',\n",
       " 'brandt cormorant',\n",
       " 'bronzed cowbird',\n",
       " 'shiny cowbird',\n",
       " 'american crow',\n",
       " 'purple finch',\n",
       " 'scissor tailed flycatcher',\n",
       " 'frigatebird',\n",
       " 'california gull',\n",
       " 'herring gull',\n",
       " 'ruby throated hummingbird',\n",
       " 'pomarine jaeger',\n",
       " 'florida jay',\n",
       " 'belted kingfisher',\n",
       " 'ringed kingfisher',\n",
       " 'pacific loon',\n",
       " 'mallard',\n",
       " 'mockingbird',\n",
       " 'hooded oriole',\n",
       " 'scott oriole',\n",
       " 'brown pelican']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vocabs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "for idx in range(101,106):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "val_check = DataLoader(cub200(args=args, base_sess = True, train=False, index = 100, index_path = 1),\n",
    "                                batch_size=64 // 2, shuffle=False, drop_last=False,\n",
    "                                num_workers=5 // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_check = DataLoader(cub200(args=args, base_sess = True, train=True, index = 100, index_path = 1),\n",
    "                                batch_size=64 // 2, shuffle=False, drop_last=False,\n",
    "                                num_workers=5 // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_check.dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 3, 3, 3, 3, 3, 3])\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5])\n",
      "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6])\n",
      "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7])\n",
      "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8])\n",
      "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9])\n",
      "tensor([ 9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
      "tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
      "tensor([11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
      "tensor([12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
      "tensor([13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14])\n",
      "tensor([14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15])\n",
      "tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17])\n",
      "tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18])\n",
      "tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19])\n",
      "tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20])\n",
      "tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21])\n",
      "tensor([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22])\n",
      "tensor([22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22,\n",
      "        23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24,\n",
      "        24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24])\n",
      "tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25,\n",
      "        25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25])\n",
      "tensor([25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26,\n",
      "        26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26])\n",
      "tensor([26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "        27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27])\n",
      "tensor([27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28])\n",
      "tensor([28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "        29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29])\n",
      "tensor([29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])\n",
      "tensor([30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "        31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31])\n",
      "tensor([32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32,\n",
      "        32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33])\n",
      "tensor([33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
      "        33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34])\n",
      "tensor([34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34,\n",
      "        34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35])\n",
      "tensor([35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35,\n",
      "        35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36])\n",
      "tensor([36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "        36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37])\n",
      "tensor([37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37,\n",
      "        37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38])\n",
      "tensor([38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38,\n",
      "        39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39])\n",
      "tensor([39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40,\n",
      "        40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40])\n",
      "tensor([40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41,\n",
      "        41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41])\n",
      "tensor([41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42,\n",
      "        42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42])\n",
      "tensor([42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "        43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43])\n",
      "tensor([43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44,\n",
      "        44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44])\n",
      "tensor([44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
      "        45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45])\n",
      "tensor([45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46,\n",
      "        46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46])\n",
      "tensor([46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "        47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47])\n",
      "tensor([48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,\n",
      "        48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49])\n",
      "tensor([49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49,\n",
      "        49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50])\n",
      "tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "        50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51])\n",
      "tensor([51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51,\n",
      "        51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52])\n",
      "tensor([52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52,\n",
      "        52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53])\n",
      "tensor([53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "        53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54])\n",
      "tensor([54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54,\n",
      "        55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55])\n",
      "tensor([55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56,\n",
      "        56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56])\n",
      "tensor([56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57,\n",
      "        57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57])\n",
      "tensor([57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58,\n",
      "        58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58])\n",
      "tensor([58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59,\n",
      "        59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59])\n",
      "tensor([59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\n",
      "        60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60])\n",
      "tensor([60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "        61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61])\n",
      "tensor([61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62,\n",
      "        62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62])\n",
      "tensor([62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63,\n",
      "        63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63])\n",
      "tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65])\n",
      "tensor([65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,\n",
      "        65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66])\n",
      "tensor([66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66,\n",
      "        66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67])\n",
      "tensor([67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
      "        67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68])\n",
      "tensor([68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68,\n",
      "        68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69])\n",
      "tensor([69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69,\n",
      "        69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70])\n",
      "tensor([70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70,\n",
      "        71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71])\n",
      "tensor([71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72,\n",
      "        72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72])\n",
      "tensor([72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73,\n",
      "        73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73])\n",
      "tensor([73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74,\n",
      "        74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74])\n",
      "tensor([74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75,\n",
      "        75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75])\n",
      "tensor([75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76,\n",
      "        76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76])\n",
      "tensor([76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n",
      "        77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77])\n",
      "tensor([77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,\n",
      "        78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78])\n",
      "tensor([78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79,\n",
      "        79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79])\n",
      "tensor([80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
      "        80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81])\n",
      "tensor([81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81,\n",
      "        81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82,\n",
      "        82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83])\n",
      "tensor([83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83,\n",
      "        83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84])\n",
      "tensor([84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84,\n",
      "        84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85])\n",
      "tensor([85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85,\n",
      "        85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86])\n",
      "tensor([86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87])\n",
      "tensor([87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88,\n",
      "        88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88])\n",
      "tensor([88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89,\n",
      "        89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89])\n",
      "tensor([89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90,\n",
      "        90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90])\n",
      "tensor([90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91,\n",
      "        91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91])\n",
      "tensor([91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92,\n",
      "        92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92])\n",
      "tensor([92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93,\n",
      "        93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93])\n",
      "tensor([93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94,\n",
      "        94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94])\n",
      "tensor([94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
      "        95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95])\n",
      "tensor([96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,\n",
      "        96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97])\n",
      "tensor([97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97,\n",
      "        97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98])\n",
      "tensor([98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98,\n",
      "        98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99])\n",
      "tensor([99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99])\n"
     ]
    }
   ],
   "source": [
    "for idx, (img,target,_) in enumerate(train_check):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black footed albatross'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_check.dataset.label2human[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = DataLoader(cub200(args=args,base_sess = False, train=True, index_path = 2),batch_size = 64,shuffle = False, drop_last=False, num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__getitem__() missing 1 required positional argument: 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B165.132.142.82/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m train_test\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m()\n",
      "\u001b[0;31mTypeError\u001b[0m: __getitem__() missing 1 required positional argument: 'item'"
     ]
    }
   ],
   "source": [
    "train_test.dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test = DataLoader(cub200(args=args,base_sess = False, train=False, index_path = 2),batch_size = 64,shuffle = False, drop_last=False, num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 101,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 102,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 103,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 104,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 105,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 106,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 107,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 108,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109,\n",
       " 109]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_test.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정리하면 support_xs 는 train img, support_ys 는 label, query_xs 는 test img, query_ys는 label vocab novel은 이번에 학습하는 train 의 label2human, vocab base에 계속 더함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_test.dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_train = DataLoader(cub200(args=args,base_sess = False, train=True, index_path = 2),batch_size = 64,shuffle = False, drop_last=False, num_workers = 5)\n",
    "\n",
    "val_test = DataLoader(cub200(args=args,base_sess = False, train=False, index_path = 2),batch_size = 64,shuffle = False, drop_last=False, num_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([57, 18, 48, 38, 27, 18, 15, 58, 14, 24, 36, 28, 13, 26, 18, 28, 43, 30,\n",
       "        52,  9, 26,  4, 10, 30, 40, 53, 45, 33, 31, 12,  3, 23])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_batch_t = val_check.dataset.imgs, val_check.dataset.targets,val_check.dataset.label2human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2864"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_batch_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_xs = train_test.dataset.imgs\n",
    "support_ys = train_test.dataset.targets\n",
    "query_xs = val_test.dataset.imgs\n",
    "query_ys = val_test.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = next(meta_valloader_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 150, 3, 84, 84])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 224, 224])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(support_xs).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(query_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 102, 102, 102,\n",
       "       102, 102, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 105,\n",
       "       105, 105, 105, 105, 106, 106, 106, 106, 106, 107, 107, 107, 107,\n",
       "       107, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(support_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 13, 13, 13, 13, 18, 18, 18, 18, 18, 58, 58, 58, 58, 58, 67, 67,\n",
       "       67, 67, 67, 78, 78, 78, 78, 78, 17, 17, 17, 17, 17, 26, 26, 26, 26,\n",
       "       26, 28, 28, 28, 28, 28, 73, 73, 73, 73, 73, 97, 97, 97, 97, 97, 43,\n",
       "       43, 43, 43, 43, 61, 61, 61, 61, 61, 85, 85, 85, 85, 85, 95, 95, 95,\n",
       "       95, 95, 99, 99, 99, 99, 99,  6,  6,  6,  6,  6, 34, 34, 34, 34, 34,\n",
       "       81, 81, 81, 81, 81, 86, 86, 86, 86, 86, 90, 90, 90, 90, 90, 20, 20,\n",
       "       20, 20, 20, 22, 22, 22, 22, 22, 25, 25, 25, 25, 25, 40, 40, 40, 40,\n",
       "       40, 71, 71, 71, 71, 71])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 3, 224, 224)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([125, 3, 84, 84])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_idx[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "dataset not supported: cub200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B165.132.142.82/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb#ch0000048vscode-remote?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m create_model(\u001b[39m'\u001b[39;49m\u001b[39mresnet18\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m100\u001b[39;49m, args, vocab\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, dataset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcub200\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.142.82/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb#ch0000048vscode-remote?line=1'>2</a>\u001b[0m basenet \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(model)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B165.132.142.82/home/hsjtjrwn/lgd/subspace-reg/incretest.ipynb#ch0000048vscode-remote?line=2'>3</a>\u001b[0m base_weight, base_bias \u001b[39m=\u001b[39m basenet\u001b[39m.\u001b[39m_get_base_weights()\n",
      "File \u001b[0;32m~/lgd/subspace-reg/models/util.py:33\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(name, n_cls, opt, vocab, dataset)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmodel \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m not supported in dataset \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name, dataset))\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdataset not supported: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(dataset))\n\u001b[1;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: dataset not supported: cub200"
     ]
    }
   ],
   "source": [
    "model = create_model('resnet18', 100, args, vocab=None, dataset='cub200')\n",
    "model.load_state_dict(ckpt['model'])\n",
    "basenet = copy.deepcopy(model).cuda()\n",
    "base_weight, base_bias = basenet._get_base_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [[1,2,3],[4,5,6],[7,8,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.rand(100,640)\n",
    "l = torch.rand(10,640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 640])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cat([k,l],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "t  = torch.nn.Parameter(t,requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(t[0:k.size(0), :]- k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_v_i = get_vocabs(train_check,val_test,val_test.dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black footed albatross',\n",
       " 'laysan albatross',\n",
       " 'sooty albatross',\n",
       " 'groove billed ani',\n",
       " 'crested auklet',\n",
       " 'least auklet',\n",
       " 'parakeet auklet',\n",
       " 'rhinoceros auklet',\n",
       " 'brewer blackbird',\n",
       " 'red winged blackbird',\n",
       " 'rusty blackbird',\n",
       " 'yellow headed blackbird',\n",
       " 'bobolink',\n",
       " 'indigo bunting',\n",
       " 'lazuli bunting',\n",
       " 'painted bunting',\n",
       " 'cardinal',\n",
       " 'spotted catbird',\n",
       " 'gray catbird',\n",
       " 'yellow breasted chat',\n",
       " 'eastern towhee',\n",
       " 'chuck will widow',\n",
       " 'brandt cormorant',\n",
       " 'red faced cormorant',\n",
       " 'pelagic cormorant',\n",
       " 'bronzed cowbird',\n",
       " 'shiny cowbird',\n",
       " 'brown creeper',\n",
       " 'american crow',\n",
       " 'fish crow',\n",
       " 'black billed cuckoo',\n",
       " 'mangrove cuckoo',\n",
       " 'yellow billed cuckoo',\n",
       " 'gray crowned rosy finch',\n",
       " 'purple finch',\n",
       " 'northern flicker',\n",
       " 'acadian flycatcher',\n",
       " 'great crested flycatcher',\n",
       " 'least flycatcher',\n",
       " 'olive sided flycatcher',\n",
       " 'scissor tailed flycatcher',\n",
       " 'vermilion flycatcher',\n",
       " 'yellow bellied flycatcher',\n",
       " 'frigatebird',\n",
       " 'northern fulmar',\n",
       " 'gadwall',\n",
       " 'american goldfinch',\n",
       " 'european goldfinch',\n",
       " 'boat tailed grackle',\n",
       " 'eared grebe',\n",
       " 'horned grebe',\n",
       " 'pied billed grebe',\n",
       " 'western grebe',\n",
       " 'blue grosbeak',\n",
       " 'evening grosbeak',\n",
       " 'pine grosbeak',\n",
       " 'rose breasted grosbeak',\n",
       " 'pigeon guillemot',\n",
       " 'california gull',\n",
       " 'glaucous winged gull',\n",
       " 'heermann gull',\n",
       " 'herring gull',\n",
       " 'ivory gull',\n",
       " 'ring billed gull',\n",
       " 'slaty backed gull',\n",
       " 'western gull',\n",
       " 'anna hummingbird',\n",
       " 'ruby throated hummingbird',\n",
       " 'rufous hummingbird',\n",
       " 'green violetear',\n",
       " 'long tailed jaeger',\n",
       " 'pomarine jaeger',\n",
       " 'blue jay',\n",
       " 'florida jay',\n",
       " 'green jay',\n",
       " 'dark eyed junco',\n",
       " 'tropical kingbird',\n",
       " 'gray kingbird',\n",
       " 'belted kingfisher',\n",
       " 'green kingfisher',\n",
       " 'pied kingfisher',\n",
       " 'ringed kingfisher',\n",
       " 'white breasted kingfisher',\n",
       " 'red legged kittiwake',\n",
       " 'horned lark',\n",
       " 'pacific loon',\n",
       " 'mallard',\n",
       " 'western meadowlark',\n",
       " 'hooded merganser',\n",
       " 'red breasted merganser',\n",
       " 'mockingbird',\n",
       " 'nighthawk',\n",
       " 'clark nutcracker',\n",
       " 'white breasted nuthatch',\n",
       " 'baltimore oriole',\n",
       " 'hooded oriole',\n",
       " 'orchard oriole',\n",
       " 'scott oriole',\n",
       " 'ovenbird',\n",
       " 'brown pelican']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_check.dataset.label2human[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black footed albatross',\n",
       " 'laysan albatross',\n",
       " 'sooty albatross',\n",
       " 'groove billed ani',\n",
       " 'crested auklet',\n",
       " 'least auklet',\n",
       " 'parakeet auklet',\n",
       " 'rhinoceros auklet',\n",
       " 'brewer blackbird',\n",
       " 'red winged blackbird',\n",
       " 'rusty blackbird',\n",
       " 'yellow headed blackbird',\n",
       " 'bobolink',\n",
       " 'indigo bunting',\n",
       " 'lazuli bunting',\n",
       " 'painted bunting',\n",
       " 'cardinal',\n",
       " 'spotted catbird',\n",
       " 'gray catbird',\n",
       " 'yellow breasted chat',\n",
       " 'eastern towhee',\n",
       " 'chuck will widow',\n",
       " 'brandt cormorant',\n",
       " 'red faced cormorant',\n",
       " 'pelagic cormorant',\n",
       " 'bronzed cowbird',\n",
       " 'shiny cowbird',\n",
       " 'brown creeper',\n",
       " 'american crow',\n",
       " 'fish crow',\n",
       " 'black billed cuckoo',\n",
       " 'mangrove cuckoo',\n",
       " 'yellow billed cuckoo',\n",
       " 'gray crowned rosy finch',\n",
       " 'purple finch',\n",
       " 'northern flicker',\n",
       " 'acadian flycatcher',\n",
       " 'great crested flycatcher',\n",
       " 'least flycatcher',\n",
       " 'olive sided flycatcher',\n",
       " 'scissor tailed flycatcher',\n",
       " 'vermilion flycatcher',\n",
       " 'yellow bellied flycatcher',\n",
       " 'frigatebird',\n",
       " 'northern fulmar',\n",
       " 'gadwall',\n",
       " 'american goldfinch',\n",
       " 'european goldfinch',\n",
       " 'boat tailed grackle',\n",
       " 'eared grebe',\n",
       " 'horned grebe',\n",
       " 'pied billed grebe',\n",
       " 'western grebe',\n",
       " 'blue grosbeak',\n",
       " 'evening grosbeak',\n",
       " 'pine grosbeak',\n",
       " 'rose breasted grosbeak',\n",
       " 'pigeon guillemot',\n",
       " 'california gull',\n",
       " 'glaucous winged gull',\n",
       " 'heermann gull',\n",
       " 'herring gull',\n",
       " 'ivory gull',\n",
       " 'ring billed gull',\n",
       " 'slaty backed gull',\n",
       " 'western gull',\n",
       " 'anna hummingbird',\n",
       " 'ruby throated hummingbird',\n",
       " 'rufous hummingbird',\n",
       " 'green violetear',\n",
       " 'long tailed jaeger',\n",
       " 'pomarine jaeger',\n",
       " 'blue jay',\n",
       " 'florida jay',\n",
       " 'green jay',\n",
       " 'dark eyed junco',\n",
       " 'tropical kingbird',\n",
       " 'gray kingbird',\n",
       " 'belted kingfisher',\n",
       " 'green kingfisher',\n",
       " 'pied kingfisher',\n",
       " 'ringed kingfisher',\n",
       " 'white breasted kingfisher',\n",
       " 'red legged kittiwake',\n",
       " 'horned lark',\n",
       " 'pacific loon',\n",
       " 'mallard',\n",
       " 'western meadowlark',\n",
       " 'hooded merganser',\n",
       " 'red breasted merganser',\n",
       " 'mockingbird',\n",
       " 'nighthawk',\n",
       " 'clark nutcracker',\n",
       " 'white breasted nuthatch',\n",
       " 'baltimore oriole',\n",
       " 'hooded oriole',\n",
       " 'orchard oriole',\n",
       " 'scott oriole',\n",
       " 'ovenbird',\n",
       " 'brown pelican',\n",
       " 'white pelican',\n",
       " 'western wood pewee',\n",
       " 'sayornis',\n",
       " 'american pipit',\n",
       " 'whip poor will',\n",
       " 'horned puffin',\n",
       " 'common raven',\n",
       " 'white necked raven',\n",
       " 'american redstart',\n",
       " 'geococcyx',\n",
       " 'loggerhead shrike',\n",
       " 'great grey shrike',\n",
       " 'baird sparrow',\n",
       " 'black throated sparrow',\n",
       " 'brewer sparrow',\n",
       " 'chipping sparrow',\n",
       " 'clay colored sparrow',\n",
       " 'house sparrow',\n",
       " 'field sparrow',\n",
       " 'fox sparrow',\n",
       " 'grasshopper sparrow',\n",
       " 'harris sparrow',\n",
       " 'henslow sparrow',\n",
       " 'le conte sparrow',\n",
       " 'lincoln sparrow',\n",
       " 'nelson sharp tailed sparrow',\n",
       " 'savannah sparrow',\n",
       " 'seaside sparrow',\n",
       " 'song sparrow',\n",
       " 'tree sparrow',\n",
       " 'vesper sparrow',\n",
       " 'white crowned sparrow',\n",
       " 'white throated sparrow',\n",
       " 'cape glossy starling',\n",
       " 'bank swallow',\n",
       " 'barn swallow',\n",
       " 'cliff swallow',\n",
       " 'tree swallow',\n",
       " 'scarlet tanager',\n",
       " 'summer tanager',\n",
       " 'artic tern',\n",
       " 'black tern',\n",
       " 'caspian tern',\n",
       " 'common tern',\n",
       " 'elegant tern',\n",
       " 'forsters tern',\n",
       " 'least tern',\n",
       " 'green tailed towhee',\n",
       " 'brown thrasher',\n",
       " 'sage thrasher',\n",
       " 'black capped vireo',\n",
       " 'blue headed vireo',\n",
       " 'philadelphia vireo',\n",
       " 'red eyed vireo',\n",
       " 'warbling vireo',\n",
       " 'white eyed vireo',\n",
       " 'yellow throated vireo',\n",
       " 'bay breasted warbler',\n",
       " 'black and white warbler',\n",
       " 'black throated blue warbler',\n",
       " 'blue winged warbler',\n",
       " 'canada warbler',\n",
       " 'cape may warbler',\n",
       " 'cerulean warbler',\n",
       " 'chestnut sided warbler',\n",
       " 'golden winged warbler',\n",
       " 'hooded warbler',\n",
       " 'kentucky warbler',\n",
       " 'magnolia warbler',\n",
       " 'mourning warbler',\n",
       " 'myrtle warbler',\n",
       " 'nashville warbler',\n",
       " 'orange crowned warbler',\n",
       " 'palm warbler',\n",
       " 'pine warbler',\n",
       " 'prairie warbler',\n",
       " 'prothonotary warbler',\n",
       " 'swainson warbler',\n",
       " 'tennessee warbler',\n",
       " 'wilson warbler',\n",
       " 'worm eating warbler',\n",
       " 'yellow warbler',\n",
       " 'northern waterthrush',\n",
       " 'louisiana waterthrush',\n",
       " 'bohemian waxwing',\n",
       " 'cedar waxwing',\n",
       " 'american three toed woodpecker',\n",
       " 'pileated woodpecker',\n",
       " 'red bellied woodpecker',\n",
       " 'red cockaded woodpecker',\n",
       " 'red headed woodpecker',\n",
       " 'downy woodpecker',\n",
       " 'bewick wren',\n",
       " 'cactus wren',\n",
       " 'carolina wren',\n",
       " 'house wren',\n",
       " 'marsh wren',\n",
       " 'rock wren',\n",
       " 'winter wren',\n",
       " 'common yellowthroat']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_v_i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_v_i = next(meta_valloader_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 3, 224, 224)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.dataset.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, _, height, width, channel = m_v_i[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([125, 3, 84, 84])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_idx[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 25, 3, 84, 84])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_v_i[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38-cuda11': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9372fc40ab880849ec92566c7793ac75dcb41f59a28c6763a82b0240e323922b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
